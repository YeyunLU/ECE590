{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U35FGLorsvPy"
   },
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKb7jk4JsvP2"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# from sklearn import datasets\n",
    "# boston = datasets.load_boston()\n",
    "# X = boston.data\n",
    "# y = boston.target\n",
    "\n",
    "# # train-test split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3MVwWTasvP9"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-h7M9dBsvQA"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        pred=score\n",
    "        return pred\n",
    "\n",
    "    def g(self,true,score):\n",
    "        g = 2*(score-true)\n",
    "        return g\n",
    "\n",
    "    def h(self,true,score):\n",
    "        h = np.ones(true.shape[0])\n",
    "        h = 2*h\n",
    "        return h\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    \n",
    "    def pred(self,score):\n",
    "        pred = 1 / (1 + np.exp(-score))\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i]>0.5:\n",
    "                pred[i]=1\n",
    "            else:\n",
    "                pred[i]=0\n",
    "        return pred\n",
    "\n",
    "    def g(self,true,score):\n",
    "        g = np.zeros(true.shape[0])\n",
    "        #g = np.dot(X_train.T, (h - y))* (1/ y_train.shape[0])\n",
    "        g = -true*np.exp(-score)/(1+np.exp(-score)) + np.exp(score)*(1-true)/(1+np.exp(score))\n",
    "        return g\n",
    "\n",
    "    def h(self,true,score):\n",
    "        h = np.zeros(true.shape[0])\n",
    "        h = true*np.exp(-score)/(1+np.exp(-score))**2 + np.exp(score)*(1-true)/(1+np.exp(score))**2\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEoGsKdSsvQC"
   },
   "outputs": [],
   "source": [
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = None,\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0.2,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "        self.idx = []\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "            tree = Tree( n_threads=self.n_threads,max_depth = self.max_depth, min_sample_split=self.min_sample_split,\n",
    "                        lamda=self.lamda,gamma=self.gamma,rf=1)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def get_bootstrap_data(self, X, Y,g,h):\n",
    "\n",
    "        n = X.shape[0]\n",
    "        Y = Y.reshape(n, 1)\n",
    "        tree_size = 100\n",
    "        X_Y = np.hstack((X, Y))\n",
    "        #np.random.shuffle(X_Y)\n",
    "        data_sets = []\n",
    "        for _ in range(self.num_trees):\n",
    "            idm = np.random.choice(n, tree_size, replace=True)\n",
    "            bootstrap_X_Y = X_Y[idm, :]\n",
    "            bootstrap_X = bootstrap_X_Y[:, :-1]\n",
    "            bootstrap_Y = bootstrap_X_Y[:, -1:]\n",
    "            temp_g = g[idm]\n",
    "            temp_h = h[idm]\n",
    "            data_sets.append([bootstrap_X, bootstrap_Y,temp_g,temp_h])\n",
    "        return data_sets\n",
    "      \n",
    "    def fit(self,train,target):\n",
    "        y_hat0 = np.ones((len(y_train),1))*np.mean(target)# classify\n",
    "        #y_hat0 = np.zeros((len(y_train),1)) # regression\n",
    "        target = np.reshape(target,(len(target),1))\n",
    "        loss = self.loss\n",
    "        g = loss.g(target,y_hat0)\n",
    "        h = loss.h(target,y_hat0)\n",
    "        n_sample, m_feature = np.shape(train)\n",
    "        sub_sets = self.get_bootstrap_data(train,target,g,h)\n",
    "        for i in range(self.num_trees):\n",
    "            sub_X, sub_Y,sub_g,sub_h = sub_sets[i]\n",
    "            idx = np.random.choice(m_feature, 7, replace=False)\n",
    "            self.idx.append(idx)\n",
    "            sub_X = sub_X[:, idx]\n",
    "            self.trees[i].fit(sub_X,sub_g,sub_h)\n",
    "            #print(\"tree\", i, \"fit complete\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self,test):\n",
    "        \n",
    "        scores = np.zeros((self.num_trees,len(test)))\n",
    "        for i in range(self.num_trees):   \n",
    "            scores[i,:] = self.trees[i].predict(test[:,self.idx[i]]).reshape(len(test))\n",
    "\n",
    "        #scores = np.array(scores).reshape(self.num_trees,len(test))\n",
    "        \n",
    "        score = np.mean(scores,axis=0)\n",
    "        return self.loss.pred(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RM50Bw26FYBZ"
   },
   "outputs": [],
   "source": [
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "   \n",
    "    Parameters:\n",
    "       n_threads: The number of threads used for fitting and predicting.\n",
    "       loss: Loss function for gradient boosting.\n",
    "           'mse' for regression task and 'log' for classfication task.\n",
    "           A child class of the loss class could be passed to implement customized loss.\n",
    "       max_depth: The maximum depth D_max of a tree.\n",
    "       min_sample_split: The minimum number of samples required to further split a node.\n",
    "       lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "       gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "       learning_rate: The learning rate eta of GBDT.\n",
    "       num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = None,\n",
    "        max_depth = 5, min_sample_split = 10, \n",
    "        lamda = 0.1, gamma = 0.2,\n",
    "        learning_rate = 0.005, num_trees = 100):\n",
    "       \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.tree = Tree(None,max_depth, min_sample_split,lamda, gamma, 0)\n",
    "      \n",
    "       \n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        y_prev = np.ones((len(y_train),1))*np.mean(target) #classify\n",
    "        #y_prev = np.zeros((len(target),1)) #regression\n",
    "        target = np.reshape(target,(len(target),1))\n",
    "        loss = self.loss\n",
    "        for i in range(self.num_trees):\n",
    "            g = loss.g(target,y_prev)\n",
    "            h = loss.h(target,y_prev)\n",
    "            self.tree.fit(train,g,h)\n",
    "            pred = self.tree.predict(train)\n",
    "            y_prev += self.learning_rate * pred\n",
    "            #print(\"tree\",i,\"fit complete\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        score=self.tree.predict(test)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CeXL1MRFYBh"
   },
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    def __init__(self, split_feature=None, split_threshold=None, data=None,\n",
    "                 left_child = None, right_child = None,weight=None):\n",
    "        self.feature = split_feature  # Index for the feature that is tested\n",
    "        self.threshold = split_threshold  # Threshold value for feature\n",
    "        self.data = data\n",
    "        self.left = left_child\n",
    "        self.right = right_child\n",
    "        self.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQdkjtt0FYBl"
   },
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0.2, rf = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = 0\n",
    "        self.root = TreeNode()\n",
    "      \n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        self.root = self.construct_tree(train, g,h,self.max_depth,depth=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def pred(self,node, row):  \n",
    "        #print('node left:',node.left,'node right:',node.right,' r:',row[node.feature],'th:',node.threshold)\n",
    "        #print(node.feature)\n",
    "        if node.left==None and node.right==None:\n",
    "            return node.weight\n",
    "        if row[node.feature] < node.threshold:\n",
    "            return self.pred(node.left, row)\n",
    "        else:\n",
    "            return self.pred(node.right, row)\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        scores = np.zeros((len(test),1))\n",
    "        \n",
    "        for i in range(len(test)):\n",
    "            #print('test',test[i,:])\n",
    "            scores[i]=self.pred(self.root,test[i,:])\n",
    "            #print('i',i,scores[i],'\\n')\n",
    "    \n",
    "        return scores\n",
    "     \n",
    "    def construct_tree(self, train, g, h, max_depth, depth=0):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        n_samples = train.shape[0]\n",
    "        \n",
    "        #print(n_samples)\n",
    "        \n",
    "        feature, threshold, gain = self.find_best_decision_rule(train,g,h)\n",
    "        \n",
    "        if n_samples < self.min_sample_split or depth>max_depth or gain<=0:\n",
    "            weight = -np.sum(g)/(np.sum(h)+self.lamda)\n",
    "            #print('wwww',weight)\n",
    "            return TreeNode(data = train,weight=weight)\n",
    "\n",
    "        \n",
    "        left = list()\n",
    "        right = list()\n",
    "        \n",
    "        left_index = list()\n",
    "        right_index = list()\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "          \n",
    "            if train[i,feature]<threshold:\n",
    "                left.append(train[i,:])\n",
    "                left_index.append(i)\n",
    "              \n",
    "            else:\n",
    "                right.append(train[i,:])\n",
    "                right_index.append(i)\n",
    "                \n",
    "        \n",
    "        left = np.array(left)\n",
    "        right = np.array(right)\n",
    "\n",
    "        if left.shape[0]==0 or right.shape[0]==0:\n",
    "            weight = -np.sum(g)/(np.sum(h)+self.lamda)\n",
    "            #print('wwww',weight)\n",
    "            return TreeNode(data = train,weight=weight)\n",
    "\n",
    "        left_child = self.construct_tree(left, g[left_index], h[left_index], max_depth, depth+1) \n",
    "        right_child = self.construct_tree(right, g[right_index], h[right_index],max_depth, depth+1)    \n",
    "        #print('ft',feature,threshold)\n",
    "        return TreeNode(split_feature = feature, split_threshold = threshold, data=train,\n",
    "                    left_child = left_child, right_child = right_child)\n",
    "\n",
    "    \n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "       \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        threshold_all, best_gain_all = self.find_threshold(g, h, train)\n",
    "        best_f_idx = np.argmax(best_gain_all)\n",
    "        feature = best_f_idx \n",
    "        threshold = threshold_all[best_f_idx]\n",
    "        gain = best_gain_all[best_f_idx]\n",
    "        #print('ggggggggggggg',gain,best_gain_all)\n",
    "        return feature, threshold, gain    \n",
    "    \n",
    "    def find_threshold(self, g, h, train):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        threshold = []\n",
    "        best_gain = []\n",
    "        #np.shape(train)\n",
    "        n_sample, m_feature = np.shape(train)\n",
    "        \n",
    "        for fe_i in range(m_feature):\n",
    "            index_sort = np.argsort(train[:,fe_i])\n",
    "            sort_g = g[index_sort]\n",
    "            sort_h = h[index_sort]\n",
    "            gain = []\n",
    "            for i in range(n_sample):\n",
    "                G_R = np.sum(sort_g[i:]) \n",
    "                G_L = np.sum(sort_g[:i])\n",
    "                H_R = np.sum(sort_h[i:]) \n",
    "                H_L = np.sum(sort_h[:i])\n",
    "                \n",
    "                ga = 1/2* ((G_L**2/ (H_L + self.lamda))+(G_R**2/ (H_R + self.lamda)) - ((G_L+G_R)**2)/(H_L+H_R+self.lamda)) - self.gamma\n",
    "                gain.append (ga)\n",
    "                #print('GR,GL<HR<HL',G_R,G_L,H_R,H_L,ga)\n",
    "            best_g_idx = np.argmax(gain)\n",
    "#             print('bgi',best_g_idx)\n",
    "            thold =  train[best_g_idx,fe_i]\n",
    "            best_g = gain[best_g_idx]\n",
    "            threshold.append(thold)\n",
    "            best_gain.append(best_g)\n",
    "            \n",
    "        return [threshold, best_gain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwHoqWivFYBs"
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    rmse = np.sqrt(np.mean(np.square(y-pred)))\n",
    "    return rmse\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    error = 0\n",
    "    for i in range(len(y)):\n",
    "        if pred[i] != y[i]:\n",
    "            error = error+1\n",
    "    return 1 - error/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF for classification on cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training data: 0.9396984924623115\n",
      "Accuracy for test data: 0.9473684210526316\n"
     ]
    }
   ],
   "source": [
    "rf = RF(num_trees=100,loss=logistic())\n",
    "rf.fit(X_train,y_train)\n",
    "pred = rf.predict(X_train)\n",
    "print('Accuracy for training data:',accuracy(pred,y_train))\n",
    "pred = rf.predict(X_test)\n",
    "print('Accuracy for test data:',accuracy(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT for classification on cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for training dataset: 0.9396984924623115\n",
      "accuracy for test dataset: 0.9064327485380117\n"
     ]
    }
   ],
   "source": [
    "gbdt = GBDT(num_trees=100,loss=logistic())\n",
    "gbdt.fit(X_train,y_train)\n",
    "pred = gbdt.predict(X_train)\n",
    "print('accuracy for training dataset:',accuracy(pred,y_train))\n",
    "pred = gbdt.predict(X_test)\n",
    "print('accuracy for test dataset:',accuracy(pred,y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GBDT34.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
